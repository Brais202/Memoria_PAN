\documentclass[12pt,a4paper]{article}
\usepackage[spanish]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{geometry}
\usepackage{titlesec}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage[nottoc]{tocbibind}
\usepackage{csquotes}
\usepackage{xurl}
\usepackage{hyperref}
\geometry{a4paper, margin=2.5cm, headheight=15pt}
\setstretch{1.3}

% Configuración de títulos
\titleformat{\section}
{\normalfont\Large\bfseries}{\thesection}{1em}{}
\titleformat{\subsection}
{\normalfont\large\bfseries}{\thesubsection}{1em}{}

% Encabezado
\pagestyle{fancy}
\fancyhf{}
\fancyhead[R]{Privacidad desde el Diseño}
\fancyfoot[C]{\thepage}

\begin{document}
	
	% Portada
	\begin{center}
		\vspace*{3cm}
		{\Huge\bfseries Privacidad desde el Diseño \\ Jurisprudencia y Casos Emblemáticos \par}
		\vspace{1.5cm}

		{\large Asignatura: Privacidad y Anonimidad \par}
		\vspace{0.5cm}
		{\large Máster en Ciberseguridad \par}
		\vspace{2cm}
		{\large Autores: \\ Estela Pillo González \\ Brais Gómez Espiñeira \\ Álvaro Caínzos Urtiaga \par}
		\vspace{0.5cm}
		{\large Fecha: \today \par}
		\vfill
	\end{center}
	
	\newpage
	
	% Índice
	\tableofcontents
	\newpage
	
	\section{Introducción}
	
	Cuando hablamos de \textit{Privacidad desde el Diseño}, nos referimos a un cambio fundamental en cómo protegemos los datos personales. Ya no se trata solo de reaccionar ante problemas, sino de integrar la privacidad desde el primer momento en el desarrollo de cualquier sistema o tecnología. En este trabajo, vamos a analizar cuatro casos que han marcado un antes y un después en nuestra comprensión de la privacidad digital, demostrando por qué es tan importante incorporar estas protecciones desde el inicio.
	
	Los casos que veremos abarcan tanto aspectos técnicos ---como la reidentificación de datos o el cifrado--- como cuestiones legales ---como el derecho al olvido o la vigilancia masiva---, dándonos una visión completa de los desafíos actuales en protección de datos.
	
	\section{Snowden y vigilancia masiva (NSA, PRISM)}
	
	Las revelaciones de documentos clasificados realizadas por el excontratista de la Agencia de Seguridad Nacional (NSA) Edward Snowden en junio de 2013 precipitaron una crisis global sobre el alcance de la vigilancia gubernamental. Este evento no solo desveló programas de espionaje como PRISM, sino que obligó a una reevaluación del equilibrio entre seguridad nacional y libertades civiles, sentando las bases para el concepto de \textbf{Privacidad desde el Diseño} aplicado al Estado.
	
	\subsection{El Dilema: La Naturaleza Invasiva del Metadato}
	
	Como respuesta a las filtraciones, el \emph{President's Review Group on Intelligence and Communications Technologies} publicó su informe de 2013, que abordó directamente el programa de recolección masiva de metadatos de telefonía (bajo la Sección 215 de la Ley FISA). El informe partió de una premisa ética crítica: la capacidad tecnológica para adquirir información no implica la legitimidad de su uso.
	
	El núcleo de la controversia se centró en la recopilación a granel (\textit{bulk collection}) de metadatos: información no de contenido (lo que se dice), sino sobre la comunicación (número de origen, destino, hora y duración). El Grupo de Revisión desmanteló la tesis de su inocuidad:
	
	\begin{itemize}
		\item \textbf{Violación de la Privacidad:} El informe concluyó que la colección masiva, al ser almacenada por hasta cinco años, generaba riesgos para la confianza pública y la privacidad personal. El metadato, una vez analizado, revela una \textbf{``gran cantidad de detalles''} sobre las ``asociaciones familiares, políticas, profesionales, religiosas y sexuales'' del individuo.
		\item \textbf{Fallas Operacionales:} El programa presentó \textbf{``problemas de cumplimiento serios y persistentes''} (\textit{noncompliance}). Se documentaron casos donde las búsquedas en la base de datos masiva se realizaban sin el estándar legal de sospecha requerido, afectando a la legalidad del programa.
	\end{itemize}
	
	\subsection{Las Demandas de Reforma y la Minimización de Datos}
	
	La principal conclusión del informe fue que la seguridad nacional podía ser protegida sin necesidad de que el gobierno almacenara metadatos de ciudadanos a granel. Esto estableció el principio de \textbf{minimización de datos} para el sector público.
	
	Las reformas institucionales clave exigidas por el informe fueron:
	\begin{enumerate}
		\item \textbf{Cese de la Colección Masiva:} El Congreso debía \textbf{terminar} el almacenamiento de metadatos a granel por parte del gobierno.
		\item \textbf{Retención Externa y Acceso Judicializado:} Los datos debían permanecer en manos de proveedores privados. El gobierno solo debería acceder a ellos mediante una \textbf{orden judicial individualizada} y justificada emitida por el Tribunal FISA.
	\end{enumerate}
	
	Este mandato se consolidó en la Ley \textbf{USA FREEDOM Act (2015)}, que restringió la capacidad de la NSA para continuar con la recolección masiva. La crisis de Snowden impulsó al sector tecnológico a adoptar el \textbf{cifrado de extremo a extremo} por defecto y consolidó la \textbf{Privacidad desde el Diseño} como un imperativo ético y técnico para contrarrestar la capacidad de vigilancia estatal.
	
	El legado final de las revelaciones se resume en el principio que el informe adoptó:
	
	\begin{quote}
		El gobierno no debe tener permitido \textbf{``recopilar y almacenar de forma masiva, sin digerir, información personal no pública sobre personas estadounidenses''}.
	\end{quote}
	
	
	\section{Casos de reidentificación de datos en EE.UU. y Europa}
	
	\subsection{Caso Netflix (EE.UU., 2006-2008)}
	
	En 2006, Netflix lanzó un concurso público ofreciendo un millón de dólares a quien mejorara su algoritmo de recomendaciones. Para ayudar a los participantes, liberaron un conjunto de datos con más de 100 millones de calificaciones de películas de unos 500.000 usuarios, quitando antes información identificativa como nombres o correos electrónicos \cite{narayanan2008}.
	
	Pero aquí viene lo interesante: unos investigadores demostraron que esta anonimización no era tan segura como parecía. Cruzando los patrones de calificaciones y fechas con información pública de IMDb, lograron identificar a varios usuarios, exponiendo así todo su historial de visualización en Netflix. Lo que este caso nos enseña es que nuestros gustos en películas pueden ser tan únicos como una huella digital, incluso en medio de millones de registros.
	
	\subsection{Caso de datos de movilidad en Europa (2013)}
	
	Un equipo de investigadores europeos se puso a estudiar datos de telefonía móvil anonimizados \cite{montjoye2013}. Lo que descubrieron es bastante sorprendente: con solo \textbf{cuatro puntos espacio-temporales} (básicamente, saber dónde estuvo alguien y a qué hora en cuatro momentos diferentes), podían identificar al 95\% de las personas en un conjunto de datos de 1.5 millones.
	
	Esto es importante porque hoy en día nuestras operadoras de telefonía y muchas aplicaciones recogen constantemente estos datos de ubicación. El estudio nos muestra claramente que simplemente agregar datos o quitar nombres no nos protege realmente cuando nuestros patrones de movimiento son tan personales.
	
	\subsection{Caso de datos médicos del NHS Reino Unido (2017)}
	
	El sistema de salud público británico (NHS) compartió datos de pacientes con Google DeepMind para proyectos de investigación médica. Aunque los datos estaban ``pseudonimizados'' (es decir, reemplazaron identificadores directos por códigos), unos investigadores encontraron puntos débiles importantes. Usando algoritmos de aprendizaje automático, demostraron que combinando solo tres datos comunes ---código postal, fecha de nacimiento y género--- ya se podía volver a identificar a personas en grandes bases de datos médicas.
	
	Este ejemplo nos recuerda por qué el GDPR considera los datos de salud como especialmente sensibles, y nos hace ver que necesitamos técnicas de protección mucho más sólidas que la simple sustitución de identificadores.
	
	\subsection{Lecciones técnicas para la Privacidad desde el Diseño}
	
	De todos estos casos, podemos sacar algunas conclusiones clave:
	
	\begin{enumerate}
		\item \textbf{La anonimización no es algo absoluto:} No existe eso de ``datos completamente anónimos'', sino diferentes niveles de riesgo que tenemos que evaluar constantemente.
		
		\item \textbf{Somos más únicos de lo que pensamos:} Casi cualquier conjunto de datos detallado contiene combinaciones de atributos que son específicas de cada persona.
		
		\item \textbf{Las técnicas tradicionales tienen límites:} Métodos como la k-anonimidad, que aseguran que cada registro sea similar a otros varios, pueden fallar cuando alguien cruza los datos con información adicional.
		
		\item \textbf{Necesitamos técnicas más avanzadas:} Enfoques como el \textit{diferencial de privacidad}, que añade cierto ``ruido'' matemático controlado a los datos, ofrecen garantías de privacidad mucho más sólidas \cite{dwork2008}.
	\end{enumerate}
	
	\section{Google Spain vs. AEPD (TJUE, derecho al olvido)}
	
	\subsection{Antecedentes y contexto del caso}
	
	Todo empezó en 2010 cuando Mario Costeja González, un ciudadano español, buscó su nombre en Google y encontró algo incómodo: enlaces a dos anuncios de 1998 en el periódico La Vanguardia que hablaban sobre la subasta de sus bienes por deudas con la Seguridad Social \cite{revollo2017}.
	
	Para Costeja González, esta información ya no tenía relevancia después de más de una década, así que pidió tanto a Google como al periódico que la eliminasen. Google se negó, lo que llevó a Costeja a presentar una reclamación ante la Agencia Española de Protección de Datos (AEPD). Cuando la AEPD le dio la razón, Google apeló y el caso terminó en el Tribunal de Justicia de la Unión Europea.
	
	\subsection{Cuestiones jurídicas fundamentales}
	
El Tribunal de Justicia de la Unión Europea tuvo que abordar tres cuestiones fundamentales en este caso. En primer lugar, se planteó si la normativa europea de protección de datos podía aplicarse a un motor de búsqueda como Google, que aunque tiene su sede central fuera de la Unión Europea, cuenta con una filial operativa dentro de su territorio. En segundo término, el Tribunal debía determinar si la actividad de indexar, almacenar y mostrar resultados de búsqueda constituía un «tratamiento» de datos personales que hiciera responsable a Google conforme a la ley. Por último, y quizás lo más relevante para los ciudadanos, el Tribunal tuvo que dilucidar si podemos ejercer un derecho a solicitar la eliminación de enlaces a información que nos afecta, incluso cuando esa información fue publicada originalmente de forma lícita por terceros, como podrían ser periódicos u otras fuentes.
	
	\subsection{La sentencia del TJUE (13 de mayo de 2014)}
	
	El Tribunal respondió que sí a las tres preguntas, estableciendo precedentes importantes:
	
	\subsubsection{Responsabilidad de los motores de búsqueda}
	
	El TJUE determinó que lo que hace un motor de búsqueda ---recolectar, indexar, almacenar y mostrar información personal--- sí cuenta como ``tratamiento de datos''. Esto significa que Google no era un simple intermediario técnico, sino responsable de lo que hacía con esos datos.
	
	\subsubsection{Nacimiento del derecho al olvido digital}
	
	La sentencia estableció que tenemos derecho a pedir a los motores de búsqueda que eliminen enlaces a información nuestra cuando:
	\begin{itemize}
		\item La información ya no es \textbf{adecuada} para los fines originales
		\item Ha perdido \textbf{relevancia} con el tiempo
		\item Resulta \textbf{excesiva} en relación con lo que se pretende
		\item No está actualizada o es \textbf{inexacta}
	\end{itemize}
	
	Pero este derecho no es absoluto. Hay que equilibrarlo caso por caso con otros derechos como la libertad de expresión o el interés público.
	
	\subsubsection{Aplicabilidad extraterritorial}
	
	El Tribunal confirmó que la ley europea se aplicaba a Google porque tenía una filial en España a través de la cual desarrollaba su actividad comercial.
	
\subsection{Impacto y consecuencias}

La sentencia del TJUE tuvo consecuencias muy prácticas e inmediatas. Tras conocerse el fallo, Google no tardó en poner en marcha un formulario específico para que los ciudadanos europeos pudieran solicitar la eliminación de enlaces a información personal. Si miramos las cifras, el impacto fue enorme: en apenas cinco años, la compañía recibió peticiones para borrar más de 3,5 millones de URLs, de las cuales aceptaron alrededor del 45\%. Esto nos muestra que el ``derecho al olvido'' no era una mera idea teórica, sino una demanda social real y palpable.

A nivel normativo, el camino que abrió este caso fue decisivo. El principio que estableció el Tribunal se materializó y amplió en el **Reglamento General de Protección de Datos (GDPR)** de 2018, donde quedó consagrado como el ``derecho de supresión'' en su artículo 17. El GDPR no solo recogió el testigo, sino que fue más allá, definiendo con mayor precisión las condiciones para ejercer este derecho y reforzando significativamente las obligaciones de todas las empresas y organizaciones que manejan nuestros datos personales.

Sin embargo, a pesar de estos avances, la sentencia dejó al descubierto y sin resolver una serie de tensiones fundamentales. El debate sigue vivo hoy entre el derecho a nuestra privacidad y el derecho colectivo a estar informados; entre la protección de datos personales y el interés público periodístico o histórico; y entre la soberanía normativa de Europa y la naturaleza global y sin fronteras de Internet. Estos dilemas no tienen una respuesta fácil, y su resolución sigue siendo uno de los grandes retos del mundo digital en el que vivimos.
	
	\subsection{Implicaciones para la Privacidad desde el Diseño}
	
	Este caso nos deja varias enseñanzas importantes para el diseño de sistemas:
	\begin{enumerate}
		\item \textbf{Diseñar pensando en el olvido:} Los sistemas deberían incluir desde el principio mecanismos para eliminar datos personales cuando sea necesario.
		
		\item \textbf{Evaluar el impacto:} Motores de búsqueda y plataformas similares deben evaluar cómo sus sistemas afectan a nuestros derechos de privacidad.
		
		\item \textbf{Hacer los algoritmos más transparentes:} La sentencia resalta la necesidad de entender mejor cómo los algoritmos deciden qué información mostrarnos y en qué orden.
	\end{enumerate}
	
	\section{Apple vs. FBI (cifrado y puertas traseras)}

	\subsection{Antecedentes: El atentado de San Bernardino y el iPhone bloqueado}
	
	El 2 de diciembre de 2015 se produjo un atentado en San Bernardino (California). Durante la investigación, el FBI recuperó un iPhone 5C utilizado por uno de los atacantes. El dispositivo estaba protegido por un PIN de cuatro dígitos y configurado para borrar sus datos tras diez intentos fallidos, lo que hacía muy arriesgada cualquier estrategia de fuerza bruta y dificultaba el acceso al contenido almacenado \cite{wikiAppleFBI}.
	
	Apple, en un primer momento, colaboró proporcionando los datos de las copias de seguridad existentes en iCloud, pero estas no contenían información reciente relevante para la investigación. Con el objetivo de intentar obtener una copia de seguridad más actual, se restableció la contraseña de iCloud asociada a la cuenta, lo que tuvo el efecto colateral de impedir el auto-backup del dispositivo hasta que la nueva contraseña se introdujese de nuevo en el propio teléfono, algo imposible mientras siguiera bloqueado \cite{wikiAppleFBI}.
	
	Esta situación llevó al FBI a solicitar una intervención más profunda por parte de Apple, desencadenando un conflicto legal de alcance global sobre los límites del cifrado, la privacidad y la seguridad nacional.
	
	
	\subsection{La petición del FBI y el fundamento legal}
	En febrero de 2016, ante la imposibilidad de acceder al teléfono con las herramientas habituales, el Gobierno solicitó una orden al amparo de la \textbf{All Writs Act}, que permite a los tribunales federales dictar órdenes ``necesarias o apropiadas'' para apoyar su jurisdicción cuando no existe un mecanismo legal específico para el caso. Esta orden exigía a Apple una ``ayuda técnica razonable'' \cite{wikiAppleFBI}. 
	
	Esta ayuda consistía específicamente en crear y firmar una versión especial del sistema operativo iOS, popularmente conocida como \textit{«GovtOS»}. Este software habría permitido:
	\begin{itemize}
		\item Desactivar las funciones de seguridad que borraban los datos tras múltiples intentos fallidos.
		\item Permitir introducir código por puerto físico, Wi-Fi, Bluetooth u otros medios.
		\item Eliminar los retardos entre intentos, para acelerar la prueba masiva de contraseñas.
	\end{itemize}
	
	La solicitud equivalía, en la práctica, a la creación de una ``puerta trasera'' (\textit{backdoor}) a medida. Aunque la orden indicaba que el software podría limitarse a un único dispositivo, el debate público se centró en la idea de que se trataba de una ``llave maestra'' reutilizable o replicable.
	
	\subsection{La respuesta de Apple y el debate público}
	
	Apple se opuso de manera rotunda y pública a la orden judicial. En una carta pública dirigida a sus clientes, Tim Cook (CEO en esos momentos) argumentó que el Gobierno estaba solicitando, en la práctica, la creación de una \textbf{puerta trasera} que debilitaría la seguridad de los usuarios, incluso si se afirmaba que se usaría solo ``una vez'' \cite{wiredCook2019}.
	
	La postura de la empresa se basaba en varios principios clave:
	\begin{enumerate}
		\item \textbf{Precedente peligroso:} Acceder a la petición sentaría un precedente legal que permitiría a gobiernos de todo el mundo exigir la debilitación de la seguridad de los productos tecnológicos.
		\item \textbf{No existe una puerta trasera solo para los ``buenos'':} Una herramienta creada para el FBI podría ser robada, filtrada o replicada, poniendo en riesgo la seguridad de todos los usuarios de Apple.
		\item \textbf{Privacidad por diseño:} El cifrado fuerte de extremo a extremo es una característica central del diseño de sus productos. Debilitarlo iría en contra de su compromiso con la privacidad del usuario, la cual no se podría garantizar en este caso.
	\end{enumerate}
	
	El caso generó un amplio debate y una tensión entre la seguridad pública y los derechos digitales. Mientras las agencias de seguridad defendían la necesidad de acceder a dispositivos en investigaciones críticas, distintas empresas tecnológicas (Google, Microsoft, Facebook), respaldaron a Apple, advirtiendo sobre los riesgos de deteriorar el cifrado para la sociedad digital \cite{eff2016}.
	
	\subsection{Resolución del conflicto: desbloqueo por un tercero y archivo del caso (marzo de 2016)}
	
	En marzo de 2016, el Gobierno solicitó aplazar la vista al informar de que un tercero había demostrado una posible forma de desbloquear el iPhone. Justo un día antes de la vista programada, el FBI retiró su solicitud contra Apple, anunciando que había logrado desbloquear el iPhone sin su ayuda \cite{wikiAppleFBI}.
	
	Reportes posteriores identificaron a \textbf{Azimuth Security} (empresa de ciberseguridad australiana) como el proveedor de la técnica utilizada para acceder al iPhone, la cual tuvo un coste alrededor del millón de dólares\cite{vergeCost2016}. El método consistió en una serie de \textit{exploits} que permitieron evitar las protecciones contra fuerza bruta, en concreto para evitar el borrado tras intentos fallidos y así automatizar la prueba de códigos de acceso \cite{vergeCondor2021,nineToFiveCondor2021}.
	
	Una vez obtenido el acceso, se reportó que el iPhone no contenía datos relevantes para la investigación del atentado. Este hecho puso en cuestión la necesidad de la extrema medida legal que se había intentado forzar \cite{vergeCondor2021}.
	
	\subsection{Implicaciones para la Privacidad desde el Diseño}
	
	Desde la perspectiva de Privacidad desde el Diseño, Apple vs. FBI es un caso emblemático por tres motivos:
	
	\begin{enumerate}
		\item \textbf{Cifrado como garantía estructural no negociable:} Apple defendió que la integridad del cifrado no es una característica opcional, sino la base de la confianza digital. Desarrollar el backdoor hubiera significado aceptar que la arquitectura de seguridad puede ser debilitada bajo coerción, lo cual es incompatible con la privacidad robusta\cite{wiredCook2019}.
		
		\item \textbf{Límites del poder estatal en la era digital:} Se demostró que las leyes centenarias, como la \textit{All Writs Act}, son herramientas contundentes y potencialmente inadecuadas para regular tecnologías complejas que afectan a la seguridad de millones de personas en todo el mundo\cite{wikiAppleFBI}.
		
		\item \textbf{El mercado de vulnerabilidades como factor del debate:} La resolución mostró que los gobiernos pueden recurrir al mercado de vulnerabilidades. Esta realidad plantea un dilema ético y de seguridad: ¿es preferible que los estados compren y acumulen exploits secretos en lugar de exigir puertas traseras legales? Ambas opciones conllevan riesgos importantes para la seguridad de la información\cite{vergeCost2016}.
	\end{enumerate}
	
	En conclusión, este caso evidenció que la privacidad depende cada vez más de decisiones de arquitectura: si se debilita el cifrado una vez, se debilita para todos. Aunque el caso se archivó sin sentencia, dejó planteado un debate vigente actualmente: o se normalizan puertas traseras, o se acepta que los Estados recurran a vulnerabilidades adquiridas en mercados externos para acceder a dispositivos. En ambos escenarios, la \textit{Privacidad desde el Diseño} aparece como la defensa más robusta.
	
	\newpage

	\begin{thebibliography}{99}
		\bibitem{narayanan2008}
		Narayanan, A. y Shmatikov, V. (2008). \emph{Robust de-anonymization of large sparse datasets}. En 2008 IEEE Symposium on Security and Privacy (pp. 111-125). DOI: 10.1109/SP.2008.33.
		
		\bibitem{montjoye2013}
		de Montjoye, Y.-A., Hidalgo, C. A., Verleysen, M. y Blondel, V. D. (2013). \emph{Unique in the crowd: The privacy bounds of human mobility}. Scientific Reports, 3(1), 1376. DOI: 10.1038/srep01376.
		
		\bibitem{montjoye2015}
		de Montjoye, Y.-A., Radaelli, L. y Singh, V. K. (2015). \emph{Unique in the shopping mall: On the reidentifiability of credit card metadata}. En 2015 IEEE European Symposium on Security and Privacy (pp. 1-6). DOI: 10.1109/EuroSP.2015.12.
		
		\bibitem{fung2018}
		Fung, B. C. M., Wang, K., Chen, R. y Yu, P. S. (2018). \emph{A critical evaluation of the article 29 working party's "opinion 05/2014 on anonymisation techniques"}. IEEE Transactions on Knowledge and Data Engineering, 30(9), 1847-1852. DOI: 10.1109/TKDE.2018.2812202.
		
		\bibitem{dwork2008}
		Dwork, C. (2008). \emph{Differential privacy: A survey of results}. En International Conference on Theory and Applications of Models of Computation (pp. 1-19). DOI: 10.1007/978-3-540-79228-4\_1.
		
		\bibitem{revollo2017}
		Revollo Fernández, D. (2017). \emph{El derecho al olvido en Internet a la luz de la jurisprudencia del Tribunal de Justicia de la Unión Europea: el caso Google Spain}. Opinión Jurídica, 16(31), 79-99. DOI: 10.22395/ojum.v16n31a4.
		
		        \bibitem{eff2016}
		Electronic Frontier Foundation (EFF). (2016). \emph{Apple Challenges FBI: All Writs Act Order (CA)}.
		Disponible en: \url{https://www.eff.org/cases/apple-challenges-fbi-all-writs-act-order}.
		
		
		\bibitem{wikiAppleFBI}
		Wikipedia (s.f.). \emph{Apple--FBI encryption dispute}. Wikipedia.
		Disponible en: \url{https://en.wikipedia.org/wiki/Apple%E2%80%93FBI_encryption_dispute}.
		
		\bibitem{wiredCook2019}
		Kahney, L. (2019, 16 de abril). \emph{The FBI Wanted a Backdoor to the iPhone. Tim Cook Said No}. WIRED.
		Disponible en: \url{https://www.wired.com/story/the-time-tim-cook-stood-his-ground-against-fbi/}.
		
		\bibitem{vergeCost2016}
		Brandom, R. (2016, 27 de abril). \emph{The FBI bought an iPhone hack, but not the right to tell anyone how it works}. The Verge.
		Disponible en: \url{https://www.theverge.com/2016/4/27/11518754/fbi-apple-iphone-hack-vulnerability-disclosure-vep}.
		
		\bibitem{vergeCondor2021}
		Clark, M. (2021, 14 de abril). \emph{Here’s how the FBI managed to get into the San Bernardino shooter’s iPhone}. The Verge.
		Disponible en: \url{https://www.theverge.com/2021/4/14/22383957/fbi-san-bernadino-iphone-hack-shooting-investigation}.
		
		\bibitem{nineToFiveCondor2021}
		Potuck, M. (2021, 14 de abril). \emph{Report reveals how little-known ‘Azimuth Security’ cracked the iPhone in the San Bernardino FBI case}. 9to5Mac.
		Disponible en: \url{https://9to5mac.com/2021/04/14/report-reveals-how-little-known-azimuth-security-cracked-the-iphone-in-the-san-bernardino-fbi-case/}.
		
	\end{thebibliography}
	
\end{document}